<!doctype html>
<html lang="">
	<head>
		<title>VTBI Week 2</title>
		<meta charset="utf-8">
        <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">

        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/css/reveal.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/css/theme/white.min.css">
        <link type="text/css" rel="stylesheet" href="index.css">

        <style>
        .container{
            display: flex;
        }
        .col{
            flex: 1;
            vertical-align: middle;
        }
        </style>
    </head>

    <body>

    	<div class="reveal">
    		<div class="slides">

    			<section data-background="linear-gradient(#e3a7aa, #65666a0a)">
    				<h2>Swin Transformers</h2>
                    <hr/>

                    <h4>VTBI Journal Club</h4>
                    
    				Matthew Guay <br/>

                    <br/>

		            <a href="https://vtbi-jc.github.io/week2" target="_blank">https://vtbi-jc.github.io/week2</a>
    			</section>



                <section data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <h2>Welcome Back!</h2>
                </section>
		    


                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Overview

                        1. A quick tour of [DeiT](http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf).

                        2. The original [Swin Transformer](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf).

                        3. [Swin Transformer V2](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.pdf).

                    </textarea>
                </section>


                <section data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <h3>Training data-efficient image transformers</h3>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Introduction

                        <div class="container">

                        <div class="col">
                            ![DeiT results overview](./img/deit.png)
                        </div>

                        <div class="col">
                        <br/>
                        - Follow-up to [ViT](https://arxiv.org/abs/2010.11929) focused on efficient learning.<br/><br/>

                        - **DeiT-B** uses ViT-B with different training to match convnet training efficiency.
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Contributions

                        - New token-based distillation for learning from a teacher convnet (RegNetY).

                        - Reaches (sorta) competitive ImageNet performance (85.2% vs 88.6%) without pretraining.

                        - Analysis of ViT training tricks.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Distillation token

                        <div class="container">

                        <div class="col">
                            ![DeiT distillation token part 1](./img/deit_distillation1.png)
                        </div>

                        <div class="col">
                        <br/><br/><br/>

                        ![DeiT distillation token part 2](./img/deit_distillation2.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Training tricks

                        <div class="container">

                        <div class="col">
                            ![DeiT training analysis part 1](./img/deit_analysis1.png)
                        </div>

                        <div class="col">
                        <br/><br/>

                        ![DeiT training analysis part 2](./img/deit_analysis2.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Conclusions

                        - Predicting labels + a convnet teacher's predictions greatly improves ViT training.

                        - Better idea of what training tricks are needed for vision transformers (most of them).

                        - Emphasis once again on low-res training + higher-res finetuning.

                    </textarea>
                </section>



                <section data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <h3>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h3>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Introduction

                        - **Goal**: Transformer architecture for general-purpose computer vision (including detection and segmentation).

                        - Dense prediction requires high image resolution and small token patches.

                        - Self-attention complexity grows like $n^2$ with token sequence length $n$.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Strategy

                        - **Restricted self-attention** on windows of patch tokens.

                        - **Patch merging** to create a spatial token hierarchy. Feature maps can replace convnet backbones.

                        - Alternating **shifted self-attention** windowing patterns allow cross-window communication.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Swin feature hierarchy

                        <div class="container">

                        <div class="col">
                            ![Swin fig1 image](./img/swin_fig1_image.png)
                        </div>

                        <div class="col">
                        <br/>

                        ![Swin fig1 text](./img/swin_fig1_text.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Swin feature hierarchy:

                        - **Patch merging**: Concatenate neighboring 2x2 $C$-dimensional tokens, project with FC layer to 0.25x $2C$-dimensional tokens.

                        - Produce pyramid of shape-$\left(\frac{H}{2^{n+2}}\times\frac{W}{2^{n+2}}\times 2^nC\right)$ features maps for $n=0,1,2,3$.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Shifted window self-attention

                        <div class="container">

                        <div class="col">
                            ![Swin fig2 image](./img/swin_fig2_image.png)
                        </div>

                        <div class="col">

                        ![Swin fig2 text](./img/swin_fig2_text.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Window multihead self-attention

                        - _Windowed multihead self-attention_ (W-MSA): Given $h\times w$ patches, choose window size $M$, compute self-attention on $M^2$ neighboring patches instead of global $h\times w$ patches. 

                        $$\Omega(\mathrm{MSA})=4hwC^2+2(hw)^2C$$
                        $$\Omega(\mathrm{W\mbox{-}MSA})=4hwC^2+2M^2hwC$$

                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Shifted window partitioning

                        - If all blocks use same windows, attention is constrained per-window.

                        - **Shifted windows**: Alternate windowing between W-MSA blocks, with overlapping patterns.

                        - Use W-MSA for standard windows, SW-MSA for shifted windows.

                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Efficient shifted batch computation

                        ![Efficient scheme for computing batches of shifted windows](./img/swin_shiftbatch.png)

                        - Restrict self-attention in mixed batches to contiguous regions.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Relative position bias

                        - Recall: ViT used 1D absolute position embeddings.

                        - Swin uses 2D **relative position bias**: $$\mathrm{Attention}(Q, K, V)=\mathrm{SoftMax}(QK^T/\sqrt{d}+B)V$$

                        - $B\in\mathbb{R}^{M^2\times M^2}$ parametrized by learned $\hat{B}\in\mathbb{R}^{(2M-1)\times(2M-1)}$.

                        - Can be interpolated for higher-res fine-tuning.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Swin transformer architecture

                        ![Swin transformer architecture](./img/swin_arch.png)

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Architecture variants

                        - Window size $M=7$, head query dimension $d=32$, MLP expansion $\alpha=4$.

                        - Swin-T: $C=96$, layer numbers $=\\{2, 2, 6, 2\\}$.

                        - Swin-S: $C=96$, layer numbers $=\\{2,2,18,2\\}$.

                        - Swin-B: $C=128$, layer numbers $=\\{2,2,18,2\\}$.

                        - Swin-L: $C=192$, layer numbers $=\\{2,2,18,2\\}$.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Experiments

                        - ImageNet-1K classification.

                        - COCO object detection.

                        - ADE20K semantic segmentation.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### ImageNet-1K Classification

                        - _Regular ImageNet-1K training_: AdamW, 300 epochs, learning rate 0.001, weight decay 0.05, batch size 1024, cosine learning rate scheduler, 20 epoch warm-up.

                        - _ImageNet-1K + ImageNet-22K pre-training_: 
                            1. (Pre-training) 90 epochs, weight decay 0.01, batch size 4096, 5-epoch warm-up.
                            2. (Fine-tuning) 30 epochs, constant learning rate $10^{-5}$, weight decay $10^{-8}$, batch size 1024.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### ImageNet-1K Classification Results

                        <div class="container">

                        <div class="col">
                            ![Classification results 1](./img/swin_class_res1.png)
                        </div>

                        <div class="col">

                        ![Classification results 2](./img/swin_class_res2.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### COCO Object Detection

                        - Feature pyramid output means Swin is a drop-in replacement for convnet backbones.

                        - Backbone comparison in multiple mmseg detection frameworks: Cascade Mask R-CNN, ATSS, RepPoints v2, Sparse RCNN.

                        - Beats previous SOTA on COCO test-dev by +2.7 box AP.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### COCO Object Detection Results

                        <div class="container">

                        <div class="col">
                        ![Detection results 1](./img/swin_detect_res1.png)
                        </div>

                        <div class="col">
                        ![Detection results 2](./img/swin_detect_res2.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### ADE20K Semantic Segmentation

                        - _ADE20K_: 150-category semantic segmentation dataset.

                        - Swin backbones + [UPerNet](http://openaccess.thecvf.com/content_ECCV_2018/papers/Tete_Xiao_Unified_Perceptual_Parsing_ECCV_2018_paper.pdf) in mmseg.

                        ![ADE20K Examples](./img/ade20k.png)
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### ADE20K Semantic Segmentation: Results

                        ![Swin segmentation results](./img/swin_seg.png)                        
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Conclusions

                        - Swin transformer introduces an efficient attention mechanism for images.

                        - Produces hierarchical feature maps compatible with dense prediction frameworks.

                        - Achieves SOTA dense prediction results and favorable speed-accuracy tradeoffs.                 
                        
                    </textarea>
                </section>



                <section data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <h3>Swin Transformer V2: Scaling Up Capacity and Resolution</h3>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Introduction

                        - **Goal**: Efficient training of larger Swin transformers on higher-resolution images.

                        - Improve stability of deep Swin transformers during training.

                        - Use several training tricks to train 3B parameter model on A100s.      
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Contributions

                        - Move $\mathrm{LayerNorm}$ normalization to post-W-MSA/MLP, from pre-.

                        - Replace dot product attention with scaled cosine attention.

                        - Replace relative position bias matrix with log-space continuous position bias MLP for improved low-res to high-res transfer.      
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Swin V2 Changes

                        <div class="container">

                        <div class="col">
                        ![Swin V2 intro 1](./img/swin2_intro1.png)
                        </div>

                        <div class="col">
                        <br/><br/>

                        ![Swin V2 intro 2](./img/swin2_intro2.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Post normalization

                        - Motivation: Activation values are unstable in large Swins.

                        - Swapping Attention/MLP and LN order helps.

                        ![Swin Layer Instability](./img/swin2_instability.png)

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Scaled cosine attention

                        - Swin v1: $$\mathrm{Sim}(q, k)=q\cdot k/\sqrt{d}$$

                        - Swin v2: $$\mathrm{Sim}(q, k)=\cos(q,k)/\tau=\left(\frac{q\cdot k}{||q|| ||k||}\right)/\tau$$

                        - $\tau$ is learned, not shared between heads and layers.

                        - Normalization helps stability, reduces small-set dominance in attention maps.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Continuous relative position bias

                        - Small meta-network that is a function of $(\Delta x, \Delta y)$, replaces relative position bias matrix.

                        - $B(\Delta x, \Delta y)=\mathcal{G}(\Delta x, \Delta y)$, where $\mathcal{G}$ is a 2-layer MLP with ReLU.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Log-spaced coordinates

                        - Translate raw relative positions to log-spaced: $$\widehat{\Delta x}=\mathrm{sign}(x)\cdot\log(1+|\Delta x|)$$ $$\widehat{\Delta y}=\mathrm{sign}(y)\cdot\log(1+|\Delta y|).$$

                        - Benefit: Lower extrapolation ratio when transferring from low-res to high-res images.

                    </textarea>
                </section>



    		</div>
    	</div>

        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/js/reveal.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/plugin/markdown/marked.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/plugin/markdown/markdown.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/plugin/math/math.js"></script>

        
        <script type="text/javascript">
            Reveal.initialize({

                math: {
                    mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js',
                    config: 'TeX-AMS_HTML-full'
                },

                // Factor of the display size that should remain empty around the content
                margin: 0.03,

                // Display controls in the bottom right corner
                controls: true,

                // Display a presentation progress bar
                progress: true,

                // Set default timing of 2 minutes per slide
                defaultTiming: 90,

                // Display the page number of the current slide
                slideNumber: true,

                // Push each slide change to the browser history
                history: false,

                // Enable keyboard shortcuts for navigation
                keyboard: true,

                // Enable the slide overview mode
                overview: true,

                // Vertical centering of slides
                center: true,

                // Enables touch navigation on devices with touch input
                touch: true,

                // Loop the presentation
                loop: false,

                // Change the presentation direction to be RTL
                rtl: false,

                // Randomizes the order of slides each time the presentation loads
                shuffle: false,

                // Turns fragments on and off globally
                fragments: true,

                // Flags if the presentation is running in an embedded mode,
                // i.e. contained within a limited portion of the screen
                embedded: false,

                // Flags if we should show a help overlay when the questionmark
                // key is pressed
                help: true,

                // Flags if speaker notes should be visible to all viewers
                showNotes: false,

                // Global override for autolaying embedded media (video/audio/iframe)
                // - null: Media will only autoplay if data-autoplay is present
                // - true: All media will autoplay, regardless of individual setting
                // - false: No media will autoplay, regardless of individual setting
                autoPlayMedia: null,

                // Number of milliseconds between automatically proceeding to the
                // next slide, disabled when set to 0, this value can be overwritten
                // by using a data-autoslide attribute on your slides
                autoSlide: 0,

                // Stop auto-sliding after user input
                autoSlideStoppable: true,

                // Use this method for navigation when auto-sliding
                autoSlideMethod: Reveal.navigateNext,

                // Enable slide navigation via mouse wheel
                mouseWheel: false,

                // Hides the address bar on mobile devices
                hideAddressBar: true,

                // Opens links in an iframe preview overlay
                previewLinks: false,

                // Transition style
                transition: 'slide', // none/fade/slide/convex/concave/zoom

                // Transition speed
                transitionSpeed: 'default', // default/fast/slow

                // Transition style for full page slide backgrounds
                backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom

                // Number of slides away from the current that are visible
                viewDistance: 3,

                // Parallax background image
                parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

                // Parallax background size
                parallaxBackgroundSize: '', // CSS syntax, e.g. "2100px 900px"

                // Number of pixels to move the parallax background per slide
                // - Calculated automatically unless specified
                // - Set to 0 to disable movement along an axis
                parallaxBackgroundHorizontal: null,
                parallaxBackgroundVertical: null,

                // The display mode that will be used to show slides
                display: 'block'
            });
        </script>

    </body>
        
</html>
