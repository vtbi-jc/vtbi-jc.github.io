<!doctype html>
<html lang="">
	<head>
		<title>VTBI Week 2</title>
		<meta charset="utf-8">
        <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">

        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/css/reveal.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/css/theme/white.min.css">
        <link type="text/css" rel="stylesheet" href="index.css">

        <style>
        .container{
            display: flex;
        }
        .col{
            flex: 1;
            vertical-align: middle;
        }
        </style>
    </head>

    <body>

    	<div class="reveal">
    		<div class="slides">

    			<section data-background="linear-gradient(#e3a7aa, #65666a0a)">
    				<h2>Swin Transformers</h2>
                    <hr/>

                    <h4>VTBI Journal Club: Week 2</h4>
                    
    				Matthew Guay <br/>

                    <br/>

		            <a href="https://vtbi-jc.github.io/week2" target="_blank">https://vtbi-jc.github.io/week2</a>
    			</section>



                <section data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <h2>Welcome Back!</h2>
                </section>
		    


                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Overview

                        1. A quick tour of [DeiT](http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf).

                        2. The original [Swin Transformer](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf).

                        3. [Swin Transformer V2](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.pdf).

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Things to think about

                        - How do we design transformers for high-res images?

                        - How do we create compute-efficient vision transformers?

                        - How do we create training-efficient vision transformers?

                        - How do we create (useful) **large** vision transformers?

                    </textarea>
                </section>


                <section data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <h3>Training data-efficient image transformers</h3>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Introduction

                        <div class="container">

                        <div class="col">
                            ![DeiT results overview](./img/deit.png)
                        </div>

                        <div class="col">
                        <br/>
                        - Follow-up to [ViT](https://arxiv.org/abs/2010.11929) focused on efficient learning.<br/><br/>

                        - **DeiT-B** uses ViT-B with different training to match convnet training efficiency.
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Contributions

                        - New token-based distillation for learning from a teacher convnet (RegNetY).

                        - Reaches (sorta) competitive ImageNet performance (85.2% vs 88.6%) without pretraining.

                        - Analysis of ViT training tricks.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Distillation token

                        <div class="container">

                        <div class="col">
                            ![DeiT distillation token part 1](./img/deit_distillation1.png)
                        </div>

                        <div class="col">
                        <br/><br/><br/>

                        ![DeiT distillation token part 2](./img/deit_distillation2.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Training tricks

                        <div class="container">

                        <div class="col">
                            ![DeiT training analysis part 1](./img/deit_analysis1.png)
                        </div>

                        <div class="col">
                        <br/><br/>

                        ![DeiT training analysis part 2](./img/deit_analysis2.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Conclusions

                        - Predicting labels + a convnet teacher's predictions greatly improves ViT training.

                        - Better idea of what training tricks are needed for vision transformers (most of them).

                        - Emphasis once again on low-res training + higher-res finetuning.

                    </textarea>
                </section>



                <section data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <h3>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h3>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Introduction

                        - **Goal**: Transformer architecture for general-purpose computer vision (including detection and segmentation).

                        - Dense prediction requires high image resolution and small token patches.

                        - Self-attention complexity grows like $n^2$ with token sequence length $n$.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Strategy

                        - **Restricted self-attention** on windows of patch tokens.

                        - **Patch merging** to create a spatial token hierarchy. Feature maps can replace convnet backbones.

                        - Alternating **shifted self-attention** windowing patterns allow cross-window communication.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Swin feature hierarchy

                        <div class="container">

                        <div class="col">
                            ![Swin fig1 image](./img/swin_fig1_image.png)
                        </div>

                        <div class="col">
                        <br/>

                        ![Swin fig1 text](./img/swin_fig1_text.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Swin feature hierarchy

                        - **Patch merging**: Concatenate neighboring 2x2 $C$-dimensional tokens, project with FC layer to 0.25x $2C$-dimensional tokens.

                        - Produce pyramid of shape-$\left(\frac{H}{2^{n+2}}\times\frac{W}{2^{n+2}}\times 2^nC\right)$ features maps for $n=0,1,2,3$.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Shifted window self-attention

                        <div class="container">

                        <div class="col">
                            ![Swin fig2 image](./img/swin_fig2_image.png)
                        </div>

                        <div class="col">

                        ![Swin fig2 text](./img/swin_fig2_text.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Window multihead self-attention

                        - _Windowed multihead self-attention_ (W-MSA): Given $h\times w$ patches, choose window size $M$, compute self-attention on $M^2$ neighboring patches instead of global $h\times w$ patches. 

                        $$\Omega(\mathrm{MSA})=4hwC^2+2(hw)^2C$$
                        $$\Omega(\mathrm{W\mbox{-}MSA})=4hwC^2+2M^2hwC$$

                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Shifted window partitioning

                        - If all blocks use same windows, information is siloed per-window.

                        - **Shifted windows**: Alternate windowing between W-MSA blocks, with overlapping patterns.

                        - Use W-MSA for standard windows, SW-MSA for shifted windows.

                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Efficient shifted batch computation

                        ![Efficient scheme for computing batches of shifted windows](./img/swin_shiftbatch.png)

                        - Restrict self-attention in mixed batches to contiguous regions.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Relative position bias

                        - Recall: ViT used 1D absolute position embeddings.

                        - Swin uses 2D **relative position bias**: $$\mathrm{Attention}(Q, K, V)=\mathrm{SoftMax}(QK^T/\sqrt{d}+B)V$$

                        - $B\in\mathbb{R}^{M^2\times M^2}$ parametrized by learned $\hat{B}\in\mathbb{R}^{(2M-1)\times(2M-1)}$.

                        - Can be interpolated for higher-res fine-tuning.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Swin transformer architecture

                        ![Swin transformer architecture](./img/swin_arch.png)

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Architecture variants

                        - Window size $M=7$, head query dimension $d=32$, MLP expansion $\alpha=4$.

                        - Swin-T: $C=96$, blocks $=\\{2, 2, 6, 2\\}$.

                        - Swin-S: $C=96$, blocks $=\\{2,2,18,2\\}$.

                        - Swin-B: $C=128$, blocks $=\\{2,2,18,2\\}$.

                        - Swin-L: $C=192$, blocks $=\\{2,2,18,2\\}$.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Experiments

                        - ImageNet-1K classification.

                        - COCO object detection.

                        - ADE20K semantic segmentation.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### ImageNet-1K Classification

                        - _Regular ImageNet-1K training_: AdamW, 300 epochs, learning rate (LR) 0.001, weight decay (WD) 0.05, batch size (BS) 1024, cosine learning rate scheduler (LRS), 20 epoch warm-up.

                        - _ImageNet-1K + ImageNet-22K pre-training_: 
                            1. (Pre-training) 90 epochs, WD 0.01, BS 4096, 5-epoch warm-up.
                            2. (Fine-tuning) 30 epochs, constant LR $10^{-5}$, WD $10^{-8}$, BS 1024.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### ImageNet-1K Classification Results

                        <div class="container">

                        <div class="col">
                            ![Classification results 1](./img/swin_class_res1.png)
                        </div>

                        <div class="col">

                        ![Classification results 2](./img/swin_class_res2.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### COCO Object Detection

                        - Feature pyramid output means Swin is a drop-in replacement for convnet backbones.

                        - Backbone comparison in multiple mmseg detection frameworks: Cascade Mask R-CNN, ATSS, RepPoints v2, Sparse RCNN.

                        - Beats previous SOTA on COCO test-dev by +2.7 box AP.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### COCO Object Detection Results

                        <div class="container">

                        <div class="col">
                        ![Detection results 1](./img/swin_detect_res1.png)
                        </div>

                        <div class="col">
                        ![Detection results 2](./img/swin_detect_res2.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### ADE20K Semantic Segmentation

                        - _ADE20K_: 150-category semantic segmentation dataset.

                        - Swin backbones + [UPerNet](http://openaccess.thecvf.com/content_ECCV_2018/papers/Tete_Xiao_Unified_Perceptual_Parsing_ECCV_2018_paper.pdf) in mmseg.

                        ![ADE20K Examples](./img/ade20k.png)
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### ADE20K Semantic Segmentation: Results

                        ![Swin segmentation results](./img/swin_seg.png)                        
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Conclusions

                        - Swin transformer introduces an efficient attention mechanism for images.

                        - Produces hierarchical feature maps compatible with dense prediction frameworks.

                        - Achieves SOTA dense prediction results and favorable speed-accuracy tradeoffs.                 
                        
                    </textarea>
                </section>



                <section data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <h3>Swin Transformer V2: Scaling Up Capacity and Resolution</h3>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Introduction

                        - **Goal**: Efficient training of larger Swin transformers on higher-resolution images.

                        - Improve stability of deep Swin transformers during training.

                        - Use several training tricks to train 3B parameter model on A100s.  

                        - Claim 40x faster training with 40x less labelled data than counterparts at Google.
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Contributions

                        - Move $\mathrm{LayerNorm}$ normalization to post-W-MSA/MLP, from pre-.

                        - Replace dot product attention with scaled cosine attention.

                        - Replace relative position bias matrix with log-space continuous position bias MLP for improved low-res to high-res transfer.      
                        
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Swin V2 Changes

                        <div class="container">

                        <div class="col">
                        ![Swin V2 intro 1](./img/swin2_intro1.png)
                        </div>

                        <div class="col">
                        <br/><br/>

                        ![Swin V2 intro 2](./img/swin2_intro2.png)
                        </div>

                        </div>

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Post normalization

                        - Motivation: Activation values are unstable in large Swins.

                        - Swapping Attention/MLP and LN order helps.

                        ![Swin Layer Instability](./img/swin2_instability.png)

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Scaled cosine attention

                        - Swin v1: $$\mathrm{Sim}(q, k)=q\cdot k/\sqrt{d}$$

                        - Swin v2: $$\mathrm{Sim}(q, k)=\cos(q,k)/\tau=\left(\frac{q\cdot k}{||q|| ||k||}\right)/\tau$$

                        - $\tau$ is learned, not shared between heads and layers.

                        - Normalization helps stability, reduces small-set dominance in attention maps.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Continuous relative position bias

                        - Small meta-network that is a function of $(\Delta x, \Delta y)$, replaces relative position bias matrix.

                        - $B(\Delta x, \Delta y)=\mathcal{G}(\Delta x, \Delta y)$, where $\mathcal{G}$ is a 2-layer MLP with ReLU.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Log-spaced coordinates

                        - Translate raw relative positions to log-spaced: $$\widehat{\Delta x}=\mathrm{sign}(x)\cdot\log(1+|\Delta x|)$$ $$\widehat{\Delta y}=\mathrm{sign}(y)\cdot\log(1+|\Delta y|).$$

                        - Benefit: Lower extrapolation ratio when transferring from low-res to high-res images.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Model configurations

                        - _SwinV2-T_: $C=96$, blocks $=\\{2, 2, 6, 2\\}$.

                        - _SwinV2-S/B/L_: $96/128/192$, $\\,\\,\\{2,2,18,2\\}$.

                        - _SwinV2-H_: $352$, $\\,\\,\\{2,2,18,2\\}$.

                        - _SwinV2-G_: $512$, $\\,\\,\\{2, 2, 42, 4\\}$.

                        - H+G have an added LN layer every 6 layers (blocks?).

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### (Large Model) Training

                        - Focus is on training high-res (1536x1536) high-capacity (3B param) models.

                        - Self-supervised pretraining + GPU memory usage tricks.

                        - **Question**: How to handle model scaling (data + compute) for biomedical imaging?

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Self-supervised Pretraining

                        - Uses [SimMIM](https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_SimMIM_A_Simple_Framework_for_Masked_Image_Modeling_CVPR_2022_paper.pdf) on ImageNet-22K-ext.

                        - Additional labeled classification pre-training on same dataset (70M images).

                        ![SimMIM Intro](./img/simmim.png)

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Training Tricks: ZeRO

                        - [Zero-Redundancy Optimizer (ZeRO)](https://arxiv.org/pdf/1910.02054.pdf) optimizes param broadcasts between parallel workers.

                        - Part of [DeepSpeed](https://www.deepspeed.ai/getting-started/), a framework for efficient large model training.

                        - GPU memory reduction with little effect on training speed.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### TT: Activation checkpointing

                        - [Activation checkpointing](https://arxiv.org/pdf/1604.06174.pdf): Trade memory usage for compute usage during training.

                        - Clear intermediate activation arrays during a forward pass, recompute them as needed during a backward pass.

                        - SwinV2 reports 30% slower training.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### TT: Sequential self-attention

                        - For large high-res models, self-attention computation becomes a memory bottleneck.

                        - "[...W]e implement self-attention computation sequentially, instead of using the previous batch computation approach"

                        - i.e. use batch size of 1 (per worker?) for self-attention layers (in first two stages)?

                        - Little impact on training speed.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### SwinV2-G: Self-supervised pre-training

                        - SimMIM on ImageNet-22K-ext (70M images) for 20 epochs at $192\times 192$ res.

                        - AdamW, cosine LRS, 30k steps linear warm-up, BS 9216, LR $1.4\times 10^{-3}$, WD 0.1, gradient clipping at 100 max.

                        - Uses random resize cropping (scales $[0.67, 1]$, aspect ratios $[3/4, 4/3]$), random flipping, color normalization.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### SwinV2-G: Supervised pre-training

                        - Still using ImageNet-22k-ext, for 30 epochs, maybe still at $192\times 192$?

                        - AdamW, cosine LRS, 20k steps linear warm-up, BS 9216, LR $1.4\times 10^{-3}$, layer-wise LR decay 0.86, WD 0.1, gradient clipping at 100 max.

                        - Uses RandAugment, random erasing, stochastic depth (ratio 0.3).

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Conclusions: Swin architecture

                        - Swin approach to vision transformers uses hierarchical windowed self-attention + relative position biases.

                        - More efficient training than ViT, better speed/accuracy tradeoffs. 

                        - Produces multiscale feature maps compatible with existing convnet backbones, useful for detection and segmentation tasks.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <textarea data-template>
                        ### Conclusions: Swin training

                        - Lots of sensitive hyperparameters and data augmentation go into vision transformer training.

                        - Pretraining on datasets larger than benchmark datasets is common: data + compute needs remain high.

                        - We start to see importance of self-supervised pretraining for large models.

                        - Large models also start requiring memory optimization tricks, check out [DeepSpeed](https://www.deepspeed.ai/).

                    </textarea>
                </section>



                <section data-background="linear-gradient(#e3a7aa, #65666a0a)">
                    <h2>Thank You!</h2>
                </section>



    		</div>
    	</div>

        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/js/reveal.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/plugin/markdown/marked.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/plugin/markdown/markdown.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/plugin/math/math.js"></script>

        
        <script type="text/javascript">
            Reveal.initialize({

                math: {
                    mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js',
                    config: 'TeX-AMS_HTML-full'
                },

                // Factor of the display size that should remain empty around the content
                margin: 0.03,

                // Display controls in the bottom right corner
                controls: true,

                // Display a presentation progress bar
                progress: true,

                // Set default timing of 2 minutes per slide
                defaultTiming: 90,

                // Display the page number of the current slide
                slideNumber: true,

                // Push each slide change to the browser history
                history: false,

                // Enable keyboard shortcuts for navigation
                keyboard: true,

                // Enable the slide overview mode
                overview: true,

                // Vertical centering of slides
                center: true,

                // Enables touch navigation on devices with touch input
                touch: true,

                // Loop the presentation
                loop: false,

                // Change the presentation direction to be RTL
                rtl: false,

                // Randomizes the order of slides each time the presentation loads
                shuffle: false,

                // Turns fragments on and off globally
                fragments: true,

                // Flags if the presentation is running in an embedded mode,
                // i.e. contained within a limited portion of the screen
                embedded: false,

                // Flags if we should show a help overlay when the questionmark
                // key is pressed
                help: true,

                // Flags if speaker notes should be visible to all viewers
                showNotes: false,

                // Global override for autolaying embedded media (video/audio/iframe)
                // - null: Media will only autoplay if data-autoplay is present
                // - true: All media will autoplay, regardless of individual setting
                // - false: No media will autoplay, regardless of individual setting
                autoPlayMedia: null,

                // Number of milliseconds between automatically proceeding to the
                // next slide, disabled when set to 0, this value can be overwritten
                // by using a data-autoslide attribute on your slides
                autoSlide: 0,

                // Stop auto-sliding after user input
                autoSlideStoppable: true,

                // Use this method for navigation when auto-sliding
                autoSlideMethod: Reveal.navigateNext,

                // Enable slide navigation via mouse wheel
                mouseWheel: false,

                // Hides the address bar on mobile devices
                hideAddressBar: true,

                // Opens links in an iframe preview overlay
                previewLinks: false,

                // Transition style
                transition: 'slide', // none/fade/slide/convex/concave/zoom

                // Transition speed
                transitionSpeed: 'default', // default/fast/slow

                // Transition style for full page slide backgrounds
                backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom

                // Number of slides away from the current that are visible
                viewDistance: 3,

                // Parallax background image
                parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

                // Parallax background size
                parallaxBackgroundSize: '', // CSS syntax, e.g. "2100px 900px"

                // Number of pixels to move the parallax background per slide
                // - Calculated automatically unless specified
                // - Set to 0 to disable movement along an axis
                parallaxBackgroundHorizontal: null,
                parallaxBackgroundVertical: null,

                // The display mode that will be used to show slides
                display: 'block'
            });
        </script>

    </body>
        
</html>
